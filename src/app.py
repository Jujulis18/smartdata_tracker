import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import streamlit as st
import pandas as pd
import time
from datetime import datetime
from io import StringIO
import logging

st.set_page_config(
    page_title="Web Scraper Pro",
    page_icon="üï∑Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialisation des variables de session
if 'scraping_results' not in st.session_state:
    st.session_state.scraping_results = None
if 'scraping_logs' not in st.session_state:
    st.session_state.scraping_logs = []
if 'articles_data' not in st.session_state:
    st.session_state.articles_data = []
if 'is_scraping' not in st.session_state:
    st.session_state.is_scraping = False

# Header principal
st.markdown("""
<div style="text-align: center; padding: 2rem; background: rgba(255, 255, 255, 0.15); 
           backdrop-filter: blur(20px); border-radius: 20px; margin-bottom: 2rem;
           box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);">
    <h1 style="color: white; font-size: 2.5rem; margin-bottom: 0.5rem;">üï∑Ô∏è Web Scraper Pro</h1>
    <p style="color: rgba(255, 255, 255, 0.8); font-size: 1.1rem;">
        Extraction automatique d'article
    </p>
</div>
""", unsafe_allow_html=True)

# Configuration du scraper
SCRAPER_CONFIG = {
    "url": "https://www.therapixel.fr/blog/",
    "locator_title": "h3 > a",
    "locator_description": ".entry-content > p",
    "locator_date": ".entry-date",
    "locator_link": ".entry-image > a",
    "locator_next_page": ".pagination .page-next",
    "category": "medical"
}

# Configuration du scraper
st.markdown("### ‚öôÔ∏è Configuration du Scraper")

col1, col2 = st.columns(2)

with col1:
    form = st.form('my_animal')
    
    url = form.text_input('URL cible:', SCRAPER_CONFIG['url'])
    sentence = form.text_input('Cat√©gorie:', SCRAPER_CONFIG['category'])
    submit = form.form_submit_button('Get locators ->')

    st.markdown("### üéØ Locators utilis√©s")
    st.code(f"""
    Titre: {SCRAPER_CONFIG['locator_title']}
    Description: {SCRAPER_CONFIG['locator_description']}
    Date: {SCRAPER_CONFIG['locator_date']}
    Pagination: {SCRAPER_CONFIG['locator_next_page']}
    """, language="css")

with col2:
    max_pages = st.number_input("Pages max", value=10, min_value=1, max_value=50)
    delay_seconds = st.number_input("D√©lai entre pages (s)", value=5, min_value=1, max_value=10)

def add_log(message, log_type="info"):
    st.session_state.scraping_logs.append({
        "message": message,
        "type": log_type
    })

def scrape_website_sync(max_pages, delay, progress_callback=None, log_callback=None):
    """
    Fonction de scraping synchrone utilisant Playwright
    """
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        if log_callback:
            log_callback("‚ùå Playwright non install√©. Installez avec: pip install playwright", "error")
            log_callback("üìã Puis ex√©cutez: playwright install", "info")
        return {"articles": [], "total_pages": 0, "total_articles": 0}
    
    articles = []
    current_page = 1
    
    with sync_playwright() as p:
        if log_callback:
            log_callback("üöÄ D√©marrage du navigateur...", "info")
        
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        try:
            if log_callback:
                log_callback(f"üåê Navigation vers {SCRAPER_CONFIG['url']}", "info")
            
            page.goto(SCRAPER_CONFIG['url'])
            
            has_more = True
            previous_first_title = ""
            previous_url = page.url
            
            while has_more and current_page <= max_pages:
                if log_callback:
                    log_callback(f"üìÑ Scraping de la page {current_page}...", "info")
                
                # Attendre que les articles se chargent
                try:
                    page.wait_for_selector(SCRAPER_CONFIG['locator_title'], timeout=10000)
                except Exception as e:
                    if log_callback:
                        log_callback(f"‚ö†Ô∏è Timeout en attendant les articles: {str(e)}", "warning")
                    break
                
                # Extraire les articles de la page courante
                page_articles = extract_articles_from_page_sync(page, SCRAPER_CONFIG, log_callback)
                
                articles.extend(page_articles)
                
                if log_callback:
                    log_callback(f"‚úÖ Page {current_page} termin√©e - {len(page_articles)} articles trouv√©s", "success")
                
                if progress_callback:
                    progress_callback(current_page, len(articles))
                
                # Stocker le premier titre pour d√©tecter les changements
                if page_articles:
                    previous_first_title = page_articles[0].get('title', '')
                
                previous_url = page.url
                
                # Tenter de cliquer sur la page suivante
                has_more = click_next_page_sync(page, SCRAPER_CONFIG['locator_next_page'], 
                                              previous_first_title, previous_url, log_callback)
                
                if has_more:
                    current_page += 1
                    time.sleep(delay)
                else:
                    if log_callback:
                        log_callback("üèÅ Plus de pages √† scraper", "info")
        
        except Exception as e:
            if log_callback:
                log_callback(f"‚ùå Erreur pendant le scraping: {str(e)}", "error")
        
        finally:
            browser.close()
    
    if log_callback:
        log_callback(f"üéâ Scraping termin√©: {len(articles)} articles sur {current_page} pages", "success")
    
    return {
        "articles": articles,
        "total_pages": current_page,
        "total_articles": len(articles)
    }

def extract_articles_from_page_sync(page, config, log_callback=None):
    """
    Extraire les articles de la page courante (version synchrone)
    """
    articles = []
    
    try:
        # R√©cup√©rer tous les titres
        title_elements = page.query_selector_all(config['locator_title'])
        
        # R√©cup√©rer les descriptions
        description_elements = page.query_selector_all(config['locator_description'])
        descriptions = []
        for desc_el in description_elements:
            desc_text = desc_el.text_content()
            descriptions.append(desc_text.strip() if desc_text else '')
        
        # R√©cup√©rer les dates
        date_elements = page.query_selector_all(config['locator_date'])
        dates = []
        for date_el in date_elements:
            date_text = date_el.text_content()
            dates.append(date_text.strip() if date_text else '')
        
        # Traiter chaque article
        for i, title_el in enumerate(title_elements):
            try:
                title = title_el.text_content()
                href = title_el.get_attribute('href')
                
                if not title or not title.strip():
                    continue
                
                # Construire l'URL compl√®te
                if href:
                    link = href
                else:
                    link = ''
                
                # R√©cup√©rer description et date si disponibles
                description = descriptions[i] if i < len(descriptions) else ''
                date = dates[i] if i < len(dates) else ''
                
                article = {
                    'title': title.strip(),
                    'description': description,
                    'url': link,
                    'date': date,
                    'category': config['category']
                }
                
                articles.append(article)
                
            except Exception as e:
                if log_callback:
                    log_callback(f"‚ö†Ô∏è Erreur lors de l'extraction d'un article: {str(e)}", "warning")
                continue
    
    except Exception as e:
        if log_callback:
            log_callback(f"‚ùå Erreur lors de l'extraction des articles: {str(e)}", "error")
    
    return articles

def click_next_page_sync(page, selector, previous_first_title, previous_url, log_callback=None):
    """
    Cliquer sur le bouton page suivante et v√©rifier le changement (version synchrone)
    """
    try:
        next_btn = page.locator(selector)
        count = next_btn.count()
        
        if count == 0:
            return False
        
        is_visible = next_btn.is_visible()
        is_enabled = next_btn.is_enabled()
        
        if not (is_visible and is_enabled):
            return False
        
        if log_callback:
            log_callback("üëÜ Clic sur page suivante...", "info")
        
        next_btn.click()
        
        # M√©thode 1: Attendre le changement d'URL
        try:
            page.wait_for_function(
                f"window.location.href !== '{previous_url}'",
                timeout=3000
            )
            return True
        except:
            pass
        
        # M√©thode 2: Attendre le changement du premier titre
        try:
            escaped_title = previous_first_title.replace("'", "\\'")
            page.wait_for_function(
                f"""
                () => {{
                    const el = document.querySelector('{SCRAPER_CONFIG["locator_title"]}');
                    const newTitle = el ? el.textContent.trim() : null;
                    return newTitle && newTitle !== '{escaped_title}';
                }}
                """,
                timeout=3000
            )
            return True
        except:
            if log_callback:
                log_callback("‚è≥ Aucun changement d√©tect√© apr√®s le clic", "warning")
            return False
    
    except Exception as e:
        if log_callback:
            log_callback(f"‚ùå Erreur lors du clic sur page suivante: {str(e)}", "error")
        return False

def generate_csv_data(articles):
    """G√©n√©rer les donn√©es CSV"""
    df = pd.DataFrame(articles)
    return df.to_csv(index=False)

# Section des contr√¥les
st.markdown("### üéÆ Contr√¥les")

col1, col2, col3 = st.columns([2, 2, 1])

with col1:
    if st.button("üöÄ Lancer le scraping", disabled=st.session_state.is_scraping, type="primary"):
        st.session_state.is_scraping = True
        st.session_state.scraping_logs = []
        st.session_state.articles_data = []
        st.rerun()

with col2:
    if st.session_state.scraping_results and not st.session_state.is_scraping:
        csv_data = generate_csv_data(st.session_state.articles_data)
        st.download_button(
            label="üì• T√©l√©charger CSV",
            data=csv_data,
            file_name=f"articles_scraped_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

# Processus de scraping
if st.session_state.is_scraping:
    st.markdown("### üìä Scraping en cours...")
    
    # Colonnes pour les r√©sultats
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üìã Console de scraping")
        log_container = st.container()
        
    with col2:
        st.markdown("#### üìÑ Articles extraits")
        articles_container = st.container()
    
    # Barre de progression
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Callbacks pour mise √† jour en temps r√©el
    def progress_callback(current_page, total_articles):
        progress = current_page / max_pages
        progress_bar.progress(progress)
        status_text.text(f"Page {current_page}/{max_pages} - {total_articles} articles trouv√©s")
        
        # Afficher les articles dans le conteneur
        if st.session_state.articles_data:
            with articles_container:
                df_preview = pd.DataFrame(st.session_state.articles_data[-5:])  # Derniers 5 articles
                st.dataframe(df_preview[['title', 'date']], use_container_width=True)
    
    def log_callback(message, log_type):
        add_log(message, log_type)
        # Afficher les logs en temps r√©el
        with log_container:
            for log in st.session_state.scraping_logs[-10:]:  # Derniers 10 logs
                st.markdown(f"""
                <div class="log-{log['type']}"> {log['message']}</div>
                """, unsafe_allow_html=True)
    
    try:
        add_log("üöÄ Initialisation du scraping...", "info")
        
        results = scrape_website_sync(max_pages, delay_seconds, progress_callback, log_callback)
        
        st.session_state.scraping_results = results
        st.session_state.articles_data = results['articles']
        add_log("üéâ Scraping termin√© avec succ√®s!", "success")
        
    except Exception as e:
        add_log(f"‚ùå Erreur: {str(e)}", "error")
    finally:
        st.session_state.is_scraping = False
        progress_bar.progress(1.0)
        status_text.text("Scraping termin√©!")
        time.sleep(2)
        st.rerun()

# Affichage des r√©sultats
if st.session_state.scraping_results and not st.session_state.is_scraping:
    results = st.session_state.scraping_results
    
    # M√©triques finales
    st.markdown("### ‚úÖ Mission accomplie ! Scraping termin√© avec succ√®s")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #166534; margin: 0;">{}</h2>
            <p style="color: #6b7280; margin: 0;">Pages scrap√©es</p>
        </div>
        """.format(results['total_pages']), unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #166534; margin: 0;">{}</h2>
            <p style="color: #6b7280; margin: 0;">Articles trouv√©s</p>
        </div>
        """.format(results['total_articles']), unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #166534; margin: 0;">CSV</h2>
            <p style="color: #6b7280; margin: 0;">Format export</p>
        </div>
        """, unsafe_allow_html=True)
    
   
    
    # Tableau des articles
    st.markdown("### üìã Articles extraits")
    if st.session_state.articles_data:
        df = pd.DataFrame(st.session_state.articles_data)
        st.dataframe(df, use_container_width=True)
    
    # Pr√©visualisation CSV
    st.markdown("### üìÑ Aper√ßu du fichier CSV g√©n√©r√©")
    if st.session_state.articles_data:
        csv_preview = generate_csv_data(st.session_state.articles_data)
        st.code(csv_preview[:1000] + "..." if len(csv_preview) > 1000 else csv_preview, language="csv")
        st.info(f"üí° Fichier CSV de {len(st.session_state.articles_data)} articles pr√™t pour l'importation")

# Console des logs
if st.session_state.scraping_logs:
    with st.expander("üìä Console de scraping d√©taill√©e"):
        for log in st.session_state.scraping_logs:
            st.markdown(f"""
            <div class="log-{log['type']}"> {log['message']}</div>
            """, unsafe_allow_html=True)

# Sidebar avec informations
with st.sidebar:
    st.markdown("### üìã Configuration actuelle")
    st.info(f"""
    **URL:** {SCRAPER_CONFIG['url']}
    
    **Pages max:** {max_pages}
    
    **D√©lai:** {delay_seconds}s
    
    **Cat√©gorie:** {SCRAPER_CONFIG['category']}
    """)
    
    if st.session_state.scraping_results:
        st.markdown("### üìä Statistiques")
        st.success(f"**Pages scrap√©es:** {st.session_state.scraping_results['total_pages']}")
        st.success(f"**Articles trouv√©s:** {st.session_state.scraping_results['total_articles']}")
    
    st.markdown("### ‚ÑπÔ∏è Instructions")
    st.markdown("""
    1. **Configurez** les param√®tres de scraping
    2. **Lancez** le processus avec le bouton
    3. **Suivez** les logs en temps r√©el
    4. **T√©l√©chargez** le fichier CSV final
    
   """)
    
   